{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jannis/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "from random import sample\n",
    "import json\n",
    "# import utils\n",
    "# from utils import *\n",
    "# from utils import fetch_rawtext_from_wiki,preprocess_text,split_classify_text,split_train_test\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "# Tokenize\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fetch_rawtext_from_wiki(subject='Maschinelles Lernen') :\n",
    "     \n",
    "    url = 'https://de.wikipedia.org/w/api.php'\n",
    "    params = {\n",
    "                'action': 'parse',\n",
    "                'page': subject,\n",
    "                'format': 'json',\n",
    "                'prop':'text',\n",
    "                'redirects':''\n",
    "            }\n",
    "    \n",
    "    response = requests.get(url, params=params)\n",
    "    data = response.json()\n",
    "    \n",
    "    raw_html = data['parse']['text']['*']\n",
    "    soup = BeautifulSoup(raw_html,'html.parser')\n",
    "    soup.find_all('p')\n",
    "    text = ''\n",
    "    \n",
    "    for p in soup.find_all('p'):\n",
    "        text += p.text\n",
    "    return text\n",
    "\n",
    "def preprocess_text(text):\n",
    "    newtext=re.sub(\"\\[.*?\\]\",\"_\",text)\n",
    "    i=0\n",
    "    newtext=newtext.replace(\"._\",\"_.\")\n",
    "    while i<1:\n",
    "        newtext=newtext.replace(\"._\",\".\")\n",
    "        i+=1\n",
    "    return newtext\n",
    "\n",
    "def classify_text(sen_text):\n",
    "    is_claim=[]\n",
    "    for i in sen_text:\n",
    "        if \"_\" in i:\n",
    "            is_claim.append(True)\n",
    "        else:\n",
    "            is_claim.append(False)\n",
    "    df=pd.DataFrame()\n",
    "    df[\"text\"]=sen_text\n",
    "    df[\"target\"]=is_claim\n",
    "\n",
    "    return df\n",
    "\n",
    "def split_text(text):\n",
    "    sen_text=text.split(\".\")\n",
    "    return sen_text\n",
    "\n",
    "def split_train_test(df):\n",
    "    train_df, test_df = train_test_split(df,test_size=.10)\n",
    "\n",
    "    return train_df,test_df\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    stop = set(stopwords.words(\"german\"))\n",
    "    filtered_words = [word.lower() for word in text.split() if word.lower() not in stop]\n",
    "    return \" \".join(filtered_words)\n",
    "\n",
    "def counter_word(text_col):\n",
    "    count = Counter()\n",
    "    for text in text_col.values:\n",
    "        for word in text.split():\n",
    "            count[word] += 1\n",
    "    return count\n",
    "def decode(sequence):\n",
    "    return \" \".join([reverse_word_index.get(idx, \"?\") for idx in sequence])\n",
    "    \n",
    "def fetch_from_fangcovid_local(number:int):\n",
    "    text=\" \"\n",
    "    save=os.getcwd()\n",
    "    os.chdir(\"/Users/jannis/Desktop/TryVSCode/fang-covid-main/articles/\")\n",
    "    json_list=sample(range(1,40000),number)\n",
    "    for i in json_list:\n",
    "        try:\n",
    "            f=open(str(i)+\".json\")\n",
    "            df=json.load(f)\n",
    "            f.close()\n",
    "        except:\n",
    "            raise FileExistsError\n",
    "        text=text+df[\"article\"]\n",
    "    return text\n",
    "def fetch_from_fangcovid_remote(number:int):\n",
    "    json_list=sample(range(1,40000),number)\n",
    "    df=pd.read_csv(\"https://github.com/justusmattern/fang-covid/blob/main/articles/0.json?raw=true\")\n",
    "    # for i in json_list:\n",
    "    #     url=\"https://github.com/justusmattern/fang-covid/tree/main/articles\"\n",
    "    return df\n",
    "# vectorize a text corpus by turning each text into a sequence of integers\n",
    "# fit only to training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19281\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Run Init Training/Test\n",
    "articles=[\"Maschinelles Lernen\", \"Medizin\",\"Wissenschaft\",\"Krankheit\",\"Prävention\",\"Diagnose\",\"Politik\",\"COVID-19\",\"COVID-19-Pandemie\",\"Epidemie\",\"Mykose\",\"Sexuell übertragbare Erkrankung\",\"Infektionskrankheit\",\"Bundestag\",\"Bundesrat\",\"Zeitung\",\"Rundfunk\",\"Verlag\",\"Politisches System der Bundesrepublik Deutschland\",\"Politisches System\",\"Massenmedien\",\"Medienwissenschaft\",\"Publikation\"]\n",
    "text=\"Auto\"\n",
    "for name in articles:\n",
    "    raw=fetch_rawtext_from_wiki(name)\n",
    "    text=text + raw\n",
    "\n",
    "text_pre=preprocess_text(text)\n",
    "text_split=split_text(text_pre)\n",
    "df=classify_text(text_split)\n",
    "df[\"text\"]=df.text.map(remove_stopwords)\n",
    "counter = counter_word(df.text)\n",
    "num_unique_words=len(counter)\n",
    "train_df,test_df = split_train_test(df)\n",
    "print(num_unique_words)\n",
    "tokenizer = Tokenizer(num_words=num_unique_words)\n",
    "tokenizer.fit_on_texts(train_df[\"text\"]) \n",
    "train_sequences = tokenizer.texts_to_sequences(train_df['text'])\n",
    "val_sequences = tokenizer.texts_to_sequences(test_df['text'])\n",
    "train_padded = pad_sequences(train_sequences, maxlen=32, padding=\"post\", truncating=\"post\")\n",
    "val_padded = pad_sequences(val_sequences, maxlen=32, padding=\"post\", truncating=\"post\")\n",
    "word_index = tokenizer.word_index\n",
    "reverse_word_index = dict([(idx, word) for (word, idx) in word_index.items()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 1s 6ms/step\n"
     ]
    }
   ],
   "source": [
    "fang_text=fetch_from_fangcovid_local(5)\n",
    "fang_pre=preprocess_text(fang_text)\n",
    "df_fang=pd.DataFrame()\n",
    "df_fang[\"sentence\"]=split_text(fang_pre)\n",
    "df_fang[\"tokenized\"]=tokenizer.texts_to_sequences(df_fang[\"sentence\"])\n",
    "fang_padded = pad_sequences(df_fang[\"tokenized\"], maxlen=32, padding=\"post\", truncating=\"post\")\n",
    "df_fang[\"predictions\"]=model.predict(fang_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              sentence  \\\n",
      "21    Der CSU-Politiker erklärte am Samstag:\\nDer S...   \n",
      "2    \\n            Nach der Schließung etlicher eur...   \n",
      "74         Erik Lesser (4/+2:12,3) landete auf Rang 16   \n",
      "75    Philipp Horn (3/2:20,0), Roman Rees (2/2:34,9...   \n",
      "140   Gemeint ist das Lockern des Lockdown für die ...   \n",
      "..                                                 ...   \n",
      "155  \\nVor einigen Tagen hatte die französische Reg...   \n",
      "175   Zur Finanzierung sind in diesem Jahr 218 Mill...   \n",
      "135   542 waren es auch im jüngsten Bulletin des Zi...   \n",
      "37                                 ), Robin Becker (45   \n",
      "105  \\nBob: Laura Nolte (Winterberg) hat sich gleic...   \n",
      "\n",
      "                                             tokenized  predictions  \n",
      "21   [917, 253, 80, 2975, 92, 4448, 383, 23, 10019,...     0.001278  \n",
      "2    [1741, 1351, 1446, 758, 279, 444, 3701, 8456, ...     0.001279  \n",
      "74                   [115, 6, 165, 63, 189, 8035, 166]     0.001281  \n",
      "75   [63, 6, 108, 101, 6, 6, 3598, 262, 1025, 115, ...     0.001283  \n",
      "140  [7855, 53, 9967, 7160, 2683, 8456, 622, 1025, ...     0.001283  \n",
      "..                                                 ...          ...  \n",
      "155  [1082, 231, 749, 8456, 9466, 160, 5017, 1528, ...     0.999700  \n",
      "175  [1872, 89, 758, 640, 465, 551, 129, 271, 102, ...     0.999700  \n",
      "135                       [289, 2051, 383, 6013, 3409]     0.999700  \n",
      "37                                              [1685]     0.999700  \n",
      "105                   [588, 722, 2529, 64, 4819, 2773]     0.999701  \n",
      "\n",
      "[241 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_fang.sort_values(\"predictions\"))\n",
    "os.chdir(\"/Users/jannis/Desktop/TryVSCode/fang-covid-main\")\n",
    "df_fang.to_csv(\"results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/jannis/Desktop/TryVSCode/fang-covid-main/articles'"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'numpy.ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [114], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m df_fang\u001b[39m=\u001b[39mpd\u001b[39m.\u001b[39mDataFrame()\n\u001b[0;32m----> 2\u001b[0m df_fang[\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m=\u001b[39mdecode(fang_padded)\n\u001b[1;32m      3\u001b[0m df_fang[\u001b[39m\"\u001b[39m\u001b[39mprediction\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m=\u001b[39mfang_predictions\n\u001b[1;32m      4\u001b[0m \u001b[39mprint\u001b[39m(df_fang)\n",
      "Cell \u001b[0;32mIn [96], line 67\u001b[0m, in \u001b[0;36mdecode\u001b[0;34m(sequence)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(sequence):\n\u001b[0;32m---> 67\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin([reverse_word_index\u001b[39m.\u001b[39mget(idx, \u001b[39m\"\u001b[39m\u001b[39m?\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m sequence])\n",
      "Cell \u001b[0;32mIn [96], line 67\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecode\u001b[39m(sequence):\n\u001b[0;32m---> 67\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin([reverse_word_index\u001b[39m.\u001b[39;49mget(idx, \u001b[39m\"\u001b[39;49m\u001b[39m?\u001b[39;49m\u001b[39m\"\u001b[39;49m) \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m sequence])\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
     ]
    }
   ],
   "source": [
    "# df_fang=pd.DataFrame()\n",
    "# df_fang[\"text\"]=decode(fang_padded)\n",
    "# df_fang[\"prediction\"]=fang_predictions\n",
    "# print(df_fang)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, 32, 32)            616992    \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 64)                24832     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 641,889\n",
      "Trainable params: 641,889\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Create LSTM model\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Embedding: https://www.tensorflow.org/tutorials/text/word_embeddings\n",
    "# Turns positive integers (indexes) into dense vectors of fixed size. (other approach could be one-hot-encoding)\n",
    "\n",
    "# Word embeddings give us a way to use an efficient, dense representation in which similar words have \n",
    "# a similar encoding. Importantly, you do not have to specify this encoding by hand. An embedding is a \n",
    "# dense vector of floating point values (the length of the vector is a parameter you specify).\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(layers.Embedding(num_unique_words, 32, input_length=32))\n",
    "\n",
    "# The layer will take as input an integer matrix of size (batch, input_length),\n",
    "# and the largest integer (i.e. word index) in the input should be no larger than num_words (vocabulary size).\n",
    "# Now model.output_shape is (None, input_length, 32), where `None` is the batch dimension.\n",
    "\n",
    "\n",
    "model.add(layers.LSTM(64, dropout=0.1))\n",
    "model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jannis/opt/anaconda3/envs/masterth-env/lib/python3.10/site-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141/141 - 6s - loss: 0.5105 - accuracy: 0.7859 - val_loss: 0.4831 - val_accuracy: 0.7996 - 6s/epoch - 44ms/step\n",
      "Epoch 2/20\n",
      "141/141 - 3s - loss: 0.3450 - accuracy: 0.8515 - val_loss: 0.5179 - val_accuracy: 0.7735 - 3s/epoch - 23ms/step\n",
      "Epoch 3/20\n",
      "141/141 - 3s - loss: 0.1277 - accuracy: 0.9565 - val_loss: 0.6583 - val_accuracy: 0.7836 - 3s/epoch - 23ms/step\n",
      "Epoch 4/20\n",
      "141/141 - 3s - loss: 0.0540 - accuracy: 0.9855 - val_loss: 0.8680 - val_accuracy: 0.7816 - 3s/epoch - 21ms/step\n",
      "Epoch 5/20\n",
      "141/141 - 3s - loss: 0.0365 - accuracy: 0.9911 - val_loss: 0.9193 - val_accuracy: 0.7735 - 3s/epoch - 21ms/step\n",
      "Epoch 6/20\n",
      "141/141 - 3s - loss: 0.0300 - accuracy: 0.9917 - val_loss: 1.0820 - val_accuracy: 0.7595 - 3s/epoch - 21ms/step\n",
      "Epoch 7/20\n",
      "141/141 - 3s - loss: 0.0290 - accuracy: 0.9931 - val_loss: 1.0956 - val_accuracy: 0.7655 - 3s/epoch - 22ms/step\n",
      "Epoch 8/20\n",
      "141/141 - 3s - loss: 0.0217 - accuracy: 0.9940 - val_loss: 1.2691 - val_accuracy: 0.7515 - 3s/epoch - 21ms/step\n",
      "Epoch 9/20\n",
      "141/141 - 3s - loss: 0.0167 - accuracy: 0.9964 - val_loss: 1.2953 - val_accuracy: 0.7575 - 3s/epoch - 22ms/step\n",
      "Epoch 10/20\n",
      "141/141 - 3s - loss: 0.0156 - accuracy: 0.9964 - val_loss: 1.3298 - val_accuracy: 0.7655 - 3s/epoch - 20ms/step\n",
      "Epoch 11/20\n",
      "141/141 - 3s - loss: 0.0175 - accuracy: 0.9962 - val_loss: 1.4633 - val_accuracy: 0.7655 - 3s/epoch - 21ms/step\n",
      "Epoch 12/20\n",
      "141/141 - 3s - loss: 0.0173 - accuracy: 0.9964 - val_loss: 1.3472 - val_accuracy: 0.7695 - 3s/epoch - 21ms/step\n",
      "Epoch 13/20\n",
      "141/141 - 3s - loss: 0.0196 - accuracy: 0.9953 - val_loss: 1.3502 - val_accuracy: 0.7575 - 3s/epoch - 21ms/step\n",
      "Epoch 14/20\n",
      "141/141 - 3s - loss: 0.0243 - accuracy: 0.9942 - val_loss: 1.0208 - val_accuracy: 0.7615 - 3s/epoch - 21ms/step\n",
      "Epoch 15/20\n",
      "141/141 - 3s - loss: 0.0149 - accuracy: 0.9971 - val_loss: 1.0855 - val_accuracy: 0.7776 - 3s/epoch - 20ms/step\n",
      "Epoch 16/20\n",
      "141/141 - 3s - loss: 0.0112 - accuracy: 0.9973 - val_loss: 1.0218 - val_accuracy: 0.7675 - 3s/epoch - 21ms/step\n",
      "Epoch 17/20\n",
      "141/141 - 3s - loss: 0.0087 - accuracy: 0.9987 - val_loss: 1.3460 - val_accuracy: 0.7776 - 3s/epoch - 20ms/step\n",
      "Epoch 18/20\n",
      "141/141 - 3s - loss: 0.0176 - accuracy: 0.9955 - val_loss: 1.3457 - val_accuracy: 0.7695 - 3s/epoch - 20ms/step\n",
      "Epoch 19/20\n",
      "141/141 - 3s - loss: 0.0144 - accuracy: 0.9969 - val_loss: 1.3738 - val_accuracy: 0.7555 - 3s/epoch - 21ms/step\n",
      "Epoch 20/20\n",
      "141/141 - 3s - loss: 0.0162 - accuracy: 0.9969 - val_loss: 1.2310 - val_accuracy: 0.7756 - 3s/epoch - 23ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fc36510fd90>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "optim = keras.optimizers.Adam(lr=0.001)\n",
    "metrics = [\"accuracy\"]\n",
    "\n",
    "model.compile(loss=loss, optimizer=optim, metrics=metrics)\n",
    "model.fit(train_padded, train_df[\"target\"], epochs=20, validation_data=(val_padded, test_df[\"target\"]), verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17574\n"
     ]
    }
   ],
   "source": [
    "text=fetch_from_fangcovid(5)\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 2 fields in line 7, saw 46\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [65], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df\u001b[39m=\u001b[39mpd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m\"\u001b[39;49m\u001b[39mhttps://raw.githubusercontent.com/justusmattern/fang-covid/main/articles/20000.json\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/masterth-env/lib/python3.10/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/masterth-env/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/masterth-env/lib/python3.10/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/masterth-env/lib/python3.10/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n\u001b[1;32m    610\u001b[0m \u001b[39mwith\u001b[39;00m parser:\n\u001b[0;32m--> 611\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\u001b[39m.\u001b[39;49mread(nrows)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/masterth-env/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1778\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1771\u001b[0m nrows \u001b[39m=\u001b[39m validate_integer(\u001b[39m\"\u001b[39m\u001b[39mnrows\u001b[39m\u001b[39m\"\u001b[39m, nrows)\n\u001b[1;32m   1772\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1773\u001b[0m     \u001b[39m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m     (\n\u001b[1;32m   1775\u001b[0m         index,\n\u001b[1;32m   1776\u001b[0m         columns,\n\u001b[1;32m   1777\u001b[0m         col_dict,\n\u001b[0;32m-> 1778\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mread(  \u001b[39m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1779\u001b[0m         nrows\n\u001b[1;32m   1780\u001b[0m     )\n\u001b[1;32m   1781\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m   1782\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/masterth-env/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py:230\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlow_memory:\n\u001b[0;32m--> 230\u001b[0m         chunks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reader\u001b[39m.\u001b[39;49mread_low_memory(nrows)\n\u001b[1;32m    231\u001b[0m         \u001b[39m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    232\u001b[0m         data \u001b[39m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/masterth-env/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:808\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/masterth-env/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:866\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/masterth-env/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:852\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/masterth-env/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:1973\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 2 fields in line 7, saw 46\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv(\"https://raw.githubusercontent.com/justusmattern/fang-covid/main/articles/20000.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Papst Franziskus unterstützt weiterhin die globale Corona-Panikmache. Täglich prasseln sich an Dramatik überbietende „Schreckensmeldungen“ auf die Bevölkerung nieder, die bei näherem Hinsehen meist die Verhältnismäßigkeit vermissen lassen.\n",
      "Papst Franziskus ließ nun verkünden, daß es auch zu Weihnachten keine Papstmessen geben werde. Obwohl die Corona-Medien die Nachricht bereitwillig in ihr tägliches Corona-Panikpensum aufnehmen, ist sie keine Sensation.\n",
      "Eine Sensation wäre es gewesen, wenn der Papst angekündigt hätte, endlich wieder die Zelebration öffentlicher Gottesdienste aufzunehmen. Unterdessen erlebt die Kirche im Verhältnis die größte Fahnenflucht ihrer Geschichte.\n",
      "Das vatikanische Staatssekretariat teilte gestern in einer Note allen beim Heiligen Stuhl akkreditierten Botschaftern mit, daß es zu Weihnachten, dem Geburtsfest Jesu Christi, keine öffentlichen päpstlichen Meßfeiern und protokollarischen Zeremonien geben wird.\n",
      "Auf den ersten Blick klingt die die Ankündigung des vatikanischen Presseamt nach einem Baustein mehr im täglichen Corona-Hype. Besorgniserregend ist es allemal, wenn man bedenkt, wieviel an Corona regelrechtes Theater ist und wie viele Menschen unter Vortäuschung falscher Prämissen gelähmt und gefügig gemacht werden.\n",
      "Allerdings wurde gestern „nur“ bestätigt, was Papst Franziskus seit Anfang März praktiziert. Ganze acht Monate wurde von ihm bereits kein öffentlicher Gottesdienst mehr zelebriert. Auch die Karwoche und Ostern opferte er den Corona-Maßnahmen.\n",
      "Als Staatsoberhaupt des Vatikanstaates ist er selbst seine eigene Regierung. Er bestimmt die Corona-Maßnahmen, unterscheidet sich jedoch in keiner Weise von den weltlichen Regierungen. Eher im Gegenteil.\n",
      "Er fordert von den Bischofskonferenzen durch sein Vorbild, den Regierungen bedingungslos zu folgen, selbst dann, wo diese in den innersten Bereich der Kirche eingreifen. In den Ländern des deutschen Sprachraumes verzichten die Regierungen nur deshalb darauf, weil sie wissen, daß die Bischofskonferenzen freiwillig noch strengere Maßnahmen auferlegen (Vatikan: Päpstliche Botschaft – Neue Weltordnung, Eine-Welt-Religion und Vereinte Nationen).\n",
      "  \n",
      "Weihnachten sind nicht vorrangig die Geschenke, sondern die heilige Liturgie. Plötzlich gilt das nicht mehr? Doch schon, aber nicht ganz … Der Staat verbietet Weihnachten im Kreis der größeren Familie zu feiern. Der Papst untersagt die Teilnahme an der heiligen Liturgie.\n",
      "Die Konsequenzen sind verheerend.\n",
      "Die Bischöfe scheint es nicht zu berühren. Sie verschanzen sich in ihren Palästen und legen mehr Wert darauf, gute Beziehungen zu den Regierenden zu pflegen (Mehrere katholische Priester enthüllen die Praxis des Satanismus im Vatikan).\n",
      "  \n",
      "Wann hat es dergleichen zuletzt gegeben? Vielleicht weiß jemand eine Antwort darauf.\n",
      "…\n",
      "Wenn Sie mehr über die heimlichen Machenschaften der Elite erfahren wollen, dann lesen Sie das brisante Enthüllungsbuch „Illuminatenblut: Die okkulten Rituale der Elite“ von Nikolas Pravda, mit einigen Artikeln die bereits von Suchmaschinen zensiert werden.\n",
      "Am 28. April erschien „Der Hollywood-Code: Kult, Satanismus und Symbolik – Wie Filme und Stars die Menschheit manipulieren“ (auch bei Amazon verfügbar), mit einem spannenden Kapitel: „Illuminati„.\n",
      "Ein handsigniertes Buch erhalten Sie für Euro 30,- (beide Bücher für Euro 60,-) inkl. Versand bei Zusendung einer Bestellung an: info@pravda-tv.com.\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: Expected 2 fields in line 6, saw 3\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [59], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df\u001b[39m=\u001b[39mfetch_from_fangcovid(\u001b[39m0\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn [58], line 3\u001b[0m, in \u001b[0;36mfetch_from_fangcovid\u001b[0;34m(number)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch_from_fangcovid\u001b[39m(number:\u001b[39mint\u001b[39m):\n\u001b[1;32m      2\u001b[0m     json_list\u001b[39m=\u001b[39msample(\u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m,\u001b[39m42000\u001b[39m),number)\n\u001b[0;32m----> 3\u001b[0m     df\u001b[39m=\u001b[39mpd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m\"\u001b[39;49m\u001b[39mhttps://github.com/justusmattern/fang-covid/blob/main/articles/0.json?raw=true\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      4\u001b[0m     \u001b[39m# for i in json_list:\u001b[39;00m\n\u001b[1;32m      5\u001b[0m     \u001b[39m#     url=\"https://github.com/justusmattern/fang-covid/tree/main/articles\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[39mreturn\u001b[39;00m df\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/masterth-env/lib/python3.10/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/masterth-env/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/masterth-env/lib/python3.10/site-packages/pandas/io/parsers/readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    935\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    936\u001b[0m     dialect,\n\u001b[1;32m    937\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    946\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m    947\u001b[0m )\n\u001b[1;32m    948\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 950\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/masterth-env/lib/python3.10/site-packages/pandas/io/parsers/readers.py:611\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    608\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n\u001b[1;32m    610\u001b[0m \u001b[39mwith\u001b[39;00m parser:\n\u001b[0;32m--> 611\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\u001b[39m.\u001b[39;49mread(nrows)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/masterth-env/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1778\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1771\u001b[0m nrows \u001b[39m=\u001b[39m validate_integer(\u001b[39m\"\u001b[39m\u001b[39mnrows\u001b[39m\u001b[39m\"\u001b[39m, nrows)\n\u001b[1;32m   1772\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1773\u001b[0m     \u001b[39m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1774\u001b[0m     (\n\u001b[1;32m   1775\u001b[0m         index,\n\u001b[1;32m   1776\u001b[0m         columns,\n\u001b[1;32m   1777\u001b[0m         col_dict,\n\u001b[0;32m-> 1778\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mread(  \u001b[39m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1779\u001b[0m         nrows\n\u001b[1;32m   1780\u001b[0m     )\n\u001b[1;32m   1781\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m   1782\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/masterth-env/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py:230\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlow_memory:\n\u001b[0;32m--> 230\u001b[0m         chunks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_reader\u001b[39m.\u001b[39;49mread_low_memory(nrows)\n\u001b[1;32m    231\u001b[0m         \u001b[39m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    232\u001b[0m         data \u001b[39m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/masterth-env/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:808\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/masterth-env/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:866\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/masterth-env/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:852\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/masterth-env/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:1973\u001b[0m, in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 2 fields in line 6, saw 3\n"
     ]
    }
   ],
   "source": [
    "df=fetch_from_fangcovid(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    nlp = spacy.load(\"de_core_news_md\")\n",
    "    doc = nlp(sentences[0])\n",
    "    print(doc.text)\n",
    "    for token in doc:\n",
    "        print(token.text, token.pos_, token.dep_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('masterth-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aeea6e7f45dc14f171f4c3978cea6ad3ef16857108b0c24174dd8089e5da7144"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
