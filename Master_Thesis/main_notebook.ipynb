{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk import StanfordPOSTagger,StanfordTagger\n",
    "import stanfordnlp\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'de_dep_news_trf'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m nlp \u001b[39m=\u001b[39m spacy\u001b[39m.\u001b[39;49mload(\u001b[39m\"\u001b[39;49m\u001b[39mde_dep_news_trf\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/masterth-env/lib/python3.10/site-packages/spacy/__init__.py:51\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\n\u001b[1;32m     31\u001b[0m     name: Union[\u001b[39mstr\u001b[39m, Path],\n\u001b[1;32m     32\u001b[0m     \u001b[39m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m     config: Union[Dict[\u001b[39mstr\u001b[39m, Any], Config] \u001b[39m=\u001b[39m util\u001b[39m.\u001b[39mSimpleFrozenDict(),\n\u001b[1;32m     37\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Language:\n\u001b[1;32m     38\u001b[0m     \u001b[39m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \n\u001b[1;32m     40\u001b[0m \u001b[39m    name (str): Package name or model path.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[39m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m     \u001b[39mreturn\u001b[39;00m util\u001b[39m.\u001b[39;49mload_model(\n\u001b[1;32m     52\u001b[0m         name, vocab\u001b[39m=\u001b[39;49mvocab, disable\u001b[39m=\u001b[39;49mdisable, exclude\u001b[39m=\u001b[39;49mexclude, config\u001b[39m=\u001b[39;49mconfig\n\u001b[1;32m     53\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/masterth-env/lib/python3.10/site-packages/spacy/util.py:427\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[1;32m    426\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE941\u001b[39m.\u001b[39mformat(name\u001b[39m=\u001b[39mname, full\u001b[39m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[39m# type: ignore[index]\u001b[39;00m\n\u001b[0;32m--> 427\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE050\u001b[39m.\u001b[39mformat(name\u001b[39m=\u001b[39mname))\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'de_dep_news_trf'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"de_dep_news_trf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'de_dep_news_trf'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m nlp \u001b[39m=\u001b[39m spacy\u001b[39m.\u001b[39;49mload(\u001b[39m\"\u001b[39;49m\u001b[39mde_dep_news_trf\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/masterth-env/lib/python3.10/site-packages/spacy/__init__.py:51\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\n\u001b[1;32m     31\u001b[0m     name: Union[\u001b[39mstr\u001b[39m, Path],\n\u001b[1;32m     32\u001b[0m     \u001b[39m*\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m     config: Union[Dict[\u001b[39mstr\u001b[39m, Any], Config] \u001b[39m=\u001b[39m util\u001b[39m.\u001b[39mSimpleFrozenDict(),\n\u001b[1;32m     37\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Language:\n\u001b[1;32m     38\u001b[0m     \u001b[39m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[1;32m     39\u001b[0m \n\u001b[1;32m     40\u001b[0m \u001b[39m    name (str): Package name or model path.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[39m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 51\u001b[0m     \u001b[39mreturn\u001b[39;00m util\u001b[39m.\u001b[39;49mload_model(\n\u001b[1;32m     52\u001b[0m         name, vocab\u001b[39m=\u001b[39;49mvocab, disable\u001b[39m=\u001b[39;49mdisable, exclude\u001b[39m=\u001b[39;49mexclude, config\u001b[39m=\u001b[39;49mconfig\n\u001b[1;32m     53\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/masterth-env/lib/python3.10/site-packages/spacy/util.py:427\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[1;32m    426\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE941\u001b[39m.\u001b[39mformat(name\u001b[39m=\u001b[39mname, full\u001b[39m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[39m# type: ignore[index]\u001b[39;00m\n\u001b[0;32m--> 427\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE050\u001b[39m.\u001b[39mformat(name\u001b[39m=\u001b[39mname))\n",
      "\u001b[0;31mOSError\u001b[0m: [E050] Can't find model 'de_dep_news_trf'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"de_dep_news_trf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using the default treebank \"de_gsd\" for language \"de\".\n",
      "Would you like to download the models for: de_gsd now? (Y/n)\n",
      "\n",
      "Default download directory: /Users/jannis/stanfordnlp_resources\n",
      "Hit enter to continue or type an alternate directory.\n",
      "\n",
      "Downloading models for: de_gsd\n",
      "Download location: /Users/jannis/stanfordnlp_resources/de_gsd_models.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 229M/229M [00:41<00:00, 5.47MB/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Download complete.  Models saved to: /Users/jannis/stanfordnlp_resources/de_gsd_models.zip\n",
      "Extracting models file for: de_gsd\n",
      "Cleaning up...Done.\n"
     ]
    }
   ],
   "source": [
    "stanfordnlp.download('de')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "pipe=stanfordnlp.Pipeline(lang='de')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'nltk.tag.stanford' from '/Users/jannis/opt/anaconda3/envs/masterth-env/lib/python3.10/site-packages/nltk/tag/stanford.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.tag.StanfordPOSTagger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jannis/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import resample\n",
    "from tensorflow import keras\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from random import sample\n",
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from spacy.lang.de import German\n",
    "from utils import *\n",
    "from sentence_classifier import SentenceClassifier\n",
    "from tokenizer_class import TokenizerClass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maschinelles Lernen\n",
      "Medizin\n",
      "Wissenschaft\n",
      "Krankheit\n",
      "Prävention\n",
      "Diagnose\n",
      "Politik\n",
      "COVID-19\n",
      "COVID-19-Pandemie\n",
      "Epidemie\n",
      "Mykose\n",
      "Sexuell übertragbare Erkrankung\n",
      "Infektionskrankheit\n",
      "Bundestag\n",
      "Bundesrat\n",
      "Zeitung\n",
      "Rundfunk\n",
      "Verlag\n",
      "Politisches System der Bundesrepublik Deutschland\n",
      "Politisches System\n",
      "Massenmedien\n",
      "Medienwissenschaft\n",
      "Publikation\n"
     ]
    }
   ],
   "source": [
    "articles=[\"Maschinelles Lernen\", \"Medizin\",\"Wissenschaft\",\"Krankheit\",\"Prävention\",\"Diagnose\",\"Politik\",\"COVID-19\",\"COVID-19-Pandemie\",\"Epidemie\",\"Mykose\",\"Sexuell übertragbare Erkrankung\",\"Infektionskrankheit\",\"Bundestag\",\"Bundesrat\",\"Zeitung\",\"Rundfunk\",\"Verlag\",\"Politisches System der Bundesrepublik Deutschland\",\"Politisches System\",\"Massenmedien\",\"Medienwissenschaft\",\"Publikation\"]\n",
    "# Loop over articles\n",
    "\n",
    "text=''\n",
    "for name in articles:\n",
    "    print(name)\n",
    "    raw=fetch_rawtext_from_wiki(name)\n",
    "    text=text + raw\n",
    "df=preprocess_classify_wiki_text(text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(                                                   text  target\n",
       " 2520  mehrere klinische studien legen nahe, „opportu...    True\n",
       " 3510  fortführung innenpolitischen reformkurses kamp...   False\n",
       " 487   beispielsweise durchführung präventionsmaßnahm...   False\n",
       " 992   zusätzlich infektionsbedingte reduktion ace2 a...    True\n",
       " 4316                                                  b   False\n",
       " ...                                                 ...     ...\n",
       " 314   dabei hielt geometrie arithmetik ungeeignet le...   False\n",
       " 282   bereits auswahl forschungsgegenstandes subjekt...   False\n",
       " 2821  wurde deutschen bundestag 1949 regierung lande...    True\n",
       " 4267  ersten publikationen neuen richtung gilt helmu...   False\n",
       " 2953  untergeordneter ebene vermitteln obersten staa...   False\n",
       " \n",
       " [3930 rows x 2 columns],\n",
       "                                                    text  target\n",
       " 3148  arbeit sitzungswoche beginnt abgeordnete berei...   False\n",
       " 62    erst laufe zeit erkannte man, indianische „med...    True\n",
       " 3698                                                  b   False\n",
       " 448   unterscheidung betrifft meist urteil kranken s...   False\n",
       " 1253  leitlinie divi rekonvaleszentenplasma krankenh...   False\n",
       " ...                                                 ...     ...\n",
       " 2851  entscheidung bundestages beim bundesverfassung...   False\n",
       " 1004  interferon alpha spielt komplexe, janus-artige...    True\n",
       " 3267  jahr 2019 neu gebildete abteilung umfasst zwei...   False\n",
       " 1916  1 beziehungsweise 1 märz wurden einreiseverbot...   False\n",
       " 2393    notstands durften wahlen referenden stattfinden   False\n",
       " \n",
       " [437 rows x 2 columns])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer=TokenizerClass()\n",
    "claim_extract=SentenceClassifier(df,tokenizer_class=tokenizer)\n",
    "claim_extract.preprocess_train_val()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-13 20:31:05.110663: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 32, 32)            620448    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                24832     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 645,345\n",
      "Trainable params: 645,345\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 32, 32)            620448    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 64)                24832     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 645,345\n",
      "Trainable params: 645,345\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n",
      "123/123 [==============================] - 7s 33ms/step - loss: 0.5460 - accuracy: 0.7537 - val_loss: 0.4916 - val_accuracy: 0.7620\n",
      "Epoch 2/20\n",
      "123/123 [==============================] - 3s 26ms/step - loss: 0.3291 - accuracy: 0.8623 - val_loss: 0.6358 - val_accuracy: 0.7643\n",
      "Epoch 3/20\n",
      "123/123 [==============================] - 3s 26ms/step - loss: 0.1160 - accuracy: 0.9649 - val_loss: 0.8184 - val_accuracy: 0.7574\n",
      "Epoch 3: early stopping\n"
     ]
    }
   ],
   "source": [
    "claim_extract.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 6ms/step\n"
     ]
    }
   ],
   "source": [
    "fang_text=fetch_from_fangcovid_local(5,10)\n",
    "#fang_pre=preprocess_text(fang_text)\n",
    "df_fang=pd.DataFrame()\n",
    "df_fang[\"text\"]=split_text(fang_text)\n",
    "df_fang[\"prediction\"]=claim_extract.predict_target(df_fang[\"text\"])\n",
    "\n",
    "# df_fang[\"tokenized\"]=tokenizer.texts_to_sequences(df_fang[\"sentence\"])\n",
    "# fang_padded = pad_sequences(df_fang[\"tokenized\"], maxlen=32, padding=\"post\", truncating=\"post\")\n",
    "# df_fang[\"predictions\"]=model.predict(fang_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                 text  prediction\n",
      "0                   Aufgrund der Befürchtungen der...    0.977051\n",
      "1   \\n            Die Gold-Futures nähern sich der...    0.987327\n",
      "2    Die Preise befinden sich zurzeit auf dem höch...    0.983878\n",
      "3   \\nGold scheint \"solide, großformatige, optimis...    0.983695\n",
      "4   \\n#Gold: streetTRACKS Gold Shares ETF (GLD) $G...    0.472253\n",
      "..                                                ...         ...\n",
      "87   Sie erinnerte daran, dass Passagiere während ...    0.606185\n",
      "88   Dieser darf nur während der Mahlzeiten entfer...    0.983620\n",
      "89  \\nNach den Regeln der russischen Verbrauchersc...    0.012023\n",
      "90   Reisenden mit über 37 Grad Körpertemperatur w...    0.009841\n",
      "91                                                       0.128586\n",
      "\n",
      "[92 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_fang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "claim_extract.save_current_test_as_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles=['Wiesentalbrücke','Mammut (Gattung)','Riemannsche Vermutung','Reichstag zu Augsburg','Deutsch-Französischer Krieg','Kantonsspital Winterthur','Femizid','Nicht-zufällige Segregation von Chromosomen','Beryllium','Massenaussterben','Covid-19','Pandemie','Sex','Homosexualität','OG Keemo','Conchita Wurst','Hochschule für Medien, Kommunikation und Wirtschaft','Europäische Union','Biographie','Donald Trump','Angstzustände','Doktortitel','BAHN-BKK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles=[\"Maschinelles Lernen\", \"Medizin\",\"Wissenschaft\",\"Krankheit\",\"Prävention\",\"Diagnose\",\"Politik\",\"COVID-19\",\"COVID-19-Pandemie\",\"Epidemie\",\"Mykose\",\"Sexuell übertragbare Erkrankung\",\"Infektionskrankheit\",\"Bundestag\",\"Bundesrat\",\"Zeitung\",\"Rundfunk\",\"Verlag\",\"Politisches System der Bundesrepublik Deutschland\",\"Politisches System\",\"Massenmedien\",\"Medienwissenschaft\",\"Publikation\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over articles\n",
    "\n",
    "text=''\n",
    "for name in articles:\n",
    "    print(name)\n",
    "    raw=fetch_rawtext_from_wiki(name)\n",
    "    text=text + raw\n",
    "df=preprocess_classify_wiki_text(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline, if \"claims\" is free text of all labeled claims \n",
    "\n",
    "split_claims=split_text(claims)\n",
    "df_claim =pd.DataFrame()\n",
    "df_claim[\"text\"]= split_claims\n",
    "df_claim=df_claim.assign(target=True)\n",
    "df=df.loc[df[\"target\"]==False]\n",
    "df=pd.concat([df,df_claim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Re-Balanced Class Sets\n",
    "df_majority = df[df.target==0]\n",
    "df_minority = df[df.target==1]\n",
    " \n",
    "# Downsample majority class\n",
    "df_majority_downsampled = resample(df_majority, \n",
    "                                 replace=False,    # sample without replacement\n",
    "                                 n_samples=2451,     # to match minority class\n",
    "                                 random_state=123) # reproducible results\n",
    " \n",
    "# Combine minority class with downsampled majority class\n",
    "df = pd.concat([df_majority_downsampled, df_minority])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fang_text=fetch_from_fangcovid_local(5,10)\n",
    "#fang_pre=preprocess_text(fang_text)\n",
    "df_fang=pd.DataFrame()\n",
    "df_fang[\"sentence\"]=split_text(fang_text)\n",
    "df_fang[\"tokenized\"]=tokenizer.texts_to_sequences(df_fang[\"sentence\"])\n",
    "fang_padded = pad_sequences(df_fang[\"tokenized\"], maxlen=32, padding=\"post\", truncating=\"post\")\n",
    "df_fang[\"predictions\"]=model.predict(fang_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_fang[\"predictions\"]=model.predict(fang_padded)\n",
    "\n",
    "print(df_fang.sort_values(\"predictions\"))\n",
    "os.chdir(\"/Users/jannis/Desktop/fang-covid-main\")\n",
    "df_fang=df_fang[['predictions','sentence']]\n",
    "df_fang.to_csv(\"results7.csv\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_count={}\n",
    "claims=\" \"\n",
    "os.chdir(\"/Users/jannis/Desktop/x-fact-main/data/x-fact\")\n",
    "with open(\"train.all.tsv\", 'r') as fp:\n",
    "    for line in fp:\n",
    "        arr = line.strip().split('\\t')\n",
    "        lang = arr[1].lower()\n",
    "        site = arr[2].lower()\n",
    "        domain = (lang, site)\n",
    "        \n",
    "\n",
    "        if domain not in label_count:\n",
    "                label_count[domain] = {}\n",
    "        if arr[0] == 'de':\n",
    "            claims = claims + arr[3]\n",
    "            print(arr[3])\n",
    "            \n",
    "\n",
    "        label = arr[-1].lower()\n",
    "\n",
    "        if label not in label_count[domain]:\n",
    "            label_count[domain][label] = 0\n",
    "        label_count[domain][label] +=1\n",
    "\n",
    "\n",
    "\n",
    "# new_map = {}\n",
    "# for key in label_count.keys():\n",
    "#     new_map[key] = {}\n",
    "#     counts = label_count[key]\n",
    "\n",
    "#     total = 0\n",
    "#     for k, v in counts.items():\n",
    "#         total += v\n",
    "\n",
    "#     for k,v in counts.items():\n",
    "#         new_map[key][k] = float(v)/total\n",
    "\n",
    "\n",
    "# print(new_map)\n",
    "\n",
    "# total = 0\n",
    "# count = 0\n",
    "# for key in new_map.keys():\n",
    "#     counts = new_map[key]\n",
    "#     take = True\n",
    "#     for k, v in counts.items():\n",
    "#         if v > 0.7:\n",
    "#             take = False\n",
    "\n",
    "#     if take:\n",
    "#         print(key)\n",
    "#         count +=1\n",
    "#         for k, v in label_count[key].items():\n",
    "#             total += v\n",
    "\n",
    "# print(total)\n",
    "# print(count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"https://raw.githubusercontent.com/justusmattern/fang-covid/main/articles/20000.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"mlsum\", \"de\")\n",
    "df_json=pd.read_json(dataset[\"train\"])\n",
    "claim=\" \"\n",
    "i=0\n",
    "while i<10000:\n",
    "    claim=claim+dataset[\"train\"][\"summary\"][randint(0,220000)]\n",
    "    i=+1\n",
    "array=dataset[\"train\"][\"summary\"][:10000]\n",
    "s=\" \".join(array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=German()\n",
    "doc=nlp(text)\n",
    "nlp.add_pipe('sentencizer')\n",
    "sents=list(doc.sents)\n",
    "print(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    nlp = spacy.load(\"de_core_news_md\")\n",
    "    doc = nlp(sentences[0])\n",
    "    print(doc.text)\n",
    "    for token in doc:\n",
    "        print(token.text, token.pos_, token.dep_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('masterth-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aeea6e7f45dc14f171f4c3978cea6ad3ef16857108b0c24174dd8089e5da7144"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
