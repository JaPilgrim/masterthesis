{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-27 13:49:45.874077: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/jannis/opt/anaconda3/envs/sentence_class/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jannis/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import resample\n",
    "from tensorflow import keras\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from random import sample\n",
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "import math\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from spacy.lang.de import German\n",
    "from utils import *\n",
    "from sentence_classifier import SentenceClassifier\n",
    "from tokenizer_class import TokenizerClass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'map' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m df_annotate \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39msentence\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mis_claim\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mto_exclude\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m----> 2\u001b[0m df_annotate[\u001b[39m'\u001b[39;49m\u001b[39msentence\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39m=\u001b[39m total_sentence_list\n\u001b[1;32m      3\u001b[0m df_annotate[\u001b[39m'\u001b[39m\u001b[39mis_claim\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m\n\u001b[1;32m      4\u001b[0m df_annotate[\u001b[39m'\u001b[39m\u001b[39mto_exclude\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/sentence_class/lib/python3.9/site-packages/pandas/core/frame.py:3978\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3975\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_setitem_array([key], value)\n\u001b[1;32m   3976\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3977\u001b[0m     \u001b[39m# set column\u001b[39;00m\n\u001b[0;32m-> 3978\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_set_item(key, value)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/sentence_class/lib/python3.9/site-packages/pandas/core/frame.py:4172\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4162\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_set_item\u001b[39m(\u001b[39mself\u001b[39m, key, value) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   4163\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4164\u001b[0m \u001b[39m    Add series to DataFrame in specified column.\u001b[39;00m\n\u001b[1;32m   4165\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4170\u001b[0m \u001b[39m    ensure homogeneity.\u001b[39;00m\n\u001b[1;32m   4171\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4172\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sanitize_column(value)\n\u001b[1;32m   4174\u001b[0m     \u001b[39mif\u001b[39;00m (\n\u001b[1;32m   4175\u001b[0m         key \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\n\u001b[1;32m   4176\u001b[0m         \u001b[39mand\u001b[39;00m value\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m   4177\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_extension_array_dtype(value)\n\u001b[1;32m   4178\u001b[0m     ):\n\u001b[1;32m   4179\u001b[0m         \u001b[39m# broadcast across multiple columns if necessary\u001b[39;00m\n\u001b[1;32m   4180\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mis_unique \u001b[39mor\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns, MultiIndex):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/sentence_class/lib/python3.9/site-packages/pandas/core/frame.py:4902\u001b[0m, in \u001b[0;36mDataFrame._sanitize_column\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   4889\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_sanitize_column\u001b[39m(\u001b[39mself\u001b[39m, value) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m ArrayLike:\n\u001b[1;32m   4890\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4891\u001b[0m \u001b[39m    Ensures new columns (which go into the BlockManager as new blocks) are\u001b[39;00m\n\u001b[1;32m   4892\u001b[0m \u001b[39m    always copied and converted into an array.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4900\u001b[0m \u001b[39m    numpy.ndarray or ExtensionArray\u001b[39;00m\n\u001b[1;32m   4901\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4902\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_ensure_valid_index(value)\n\u001b[1;32m   4904\u001b[0m     \u001b[39m# We can get there through isetitem with a DataFrame\u001b[39;00m\n\u001b[1;32m   4905\u001b[0m     \u001b[39m# or through loc single_block_path\u001b[39;00m\n\u001b[1;32m   4906\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(value, DataFrame):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/sentence_class/lib/python3.9/site-packages/pandas/core/frame.py:4237\u001b[0m, in \u001b[0;36mDataFrame._ensure_valid_index\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   4232\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4233\u001b[0m \u001b[39mEnsure that if we don't have an index, that we can create one from the\u001b[39;00m\n\u001b[1;32m   4234\u001b[0m \u001b[39mpassed value.\u001b[39;00m\n\u001b[1;32m   4235\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4236\u001b[0m \u001b[39m# GH5632, make sure that we are a Series convertible\u001b[39;00m\n\u001b[0;32m-> 4237\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindex) \u001b[39mand\u001b[39;00m is_list_like(value) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39;49m(value):\n\u001b[1;32m   4238\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(value, DataFrame):\n\u001b[1;32m   4239\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'map' has no len()"
     ]
    }
   ],
   "source": [
    "df_annotate = pd.DataFrame(columns=['sentence','is_claim','to_exclude'])\n",
    "df_annotate['sentence'] = total_sentence_list\n",
    "df_annotate['is_claim'] = 2\n",
    "df_annotate['to_exclude'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_annotate=df_annotate[['sentence','is_claim','to_exclude']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              sentence  is_claim  to_exclude\n",
      "0    Diese Auffassung wird durch eine Ende Juni ver...         2           0\n",
      "1    Ohnehin hat man die derzeitige Krise raffinier...         2           0\n",
      "2    „Im Moment ist nur Abstand Ausdruck von Fürsor...         2           0\n",
      "3                                           Geht nicht         2           0\n",
      "4    Die Weltbank, die Bank des Vatikans, die Bank ...         2           0\n",
      "..                                                 ...       ...         ...\n",
      "101  Warum wird dagegen nicht energischer vorgegang...         2           0\n",
      "102  Bildungssenator Ties Rabe sprach sich auch dan...         2           0\n",
      "103  Momentan \\nmüssen die Menschen ja nicht mehr u...         2           0\n",
      "104  \\nSchon im Februar warnte die Weltgesundheitso...         2           0\n",
      "105  Die stereotypen Frauenfiguren sind reine Sexob...         2           0\n",
      "\n",
      "[106 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_annotate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<map object at 0x7fd2bb3f31f0>\n"
     ]
    }
   ],
   "source": [
    "print(total_sentence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in total_sentence_list:\n",
    "    s = s.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_sentence_list = map(lambda s:s.strip(),total_sentence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_annotate.to_csv(f\"annotate_106_zerotrue.csv\",sep='!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maschinelles Lernen\n",
      "Medizin\n",
      "Wissenschaft\n",
      "Krankheit\n",
      "Prävention\n",
      "Diagnose\n",
      "Politik\n",
      "COVID-19\n",
      "COVID-19-Pandemie\n",
      "Epidemie\n",
      "Mykose\n",
      "Sexuell übertragbare Erkrankung\n",
      "Infektionskrankheit\n",
      "Bundestag\n",
      "Bundesrat\n",
      "Zeitung\n",
      "Rundfunk\n",
      "Verlag\n",
      "Politisches System der Bundesrepublik Deutschland\n",
      "Politisches System\n",
      "Massenmedien\n",
      "Medienwissenschaft\n",
      "Publikation\n"
     ]
    }
   ],
   "source": [
    "articles=[\"Maschinelles Lernen\", \"Medizin\",\"Wissenschaft\",\"Krankheit\",\"Prävention\",\"Diagnose\",\"Politik\",\"COVID-19\",\"COVID-19-Pandemie\",\"Epidemie\",\"Mykose\",\"Sexuell übertragbare Erkrankung\",\"Infektionskrankheit\",\"Bundestag\",\"Bundesrat\",\"Zeitung\",\"Rundfunk\",\"Verlag\",\"Politisches System der Bundesrepublik Deutschland\",\"Politisches System\",\"Massenmedien\",\"Medienwissenschaft\",\"Publikation\"]\n",
    "# Loop over articles\n",
    "\n",
    "text=''\n",
    "for name in articles:\n",
    "    print(name)\n",
    "    raw=fetch_rawtext_from_wiki(name)\n",
    "    text=text + raw\n",
    "df=preprocess_classify_wiki_text(text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got multiple values for argument 'tokenizer_class'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m tokenizer\u001b[39m=\u001b[39mTokenizerClass()\n\u001b[0;32m----> 2\u001b[0m claim_extract\u001b[39m=\u001b[39mSentenceClassifier(df,tokenizer_class\u001b[39m=\u001b[39;49mtokenizer)\n\u001b[1;32m      3\u001b[0m claim_extract\u001b[39m.\u001b[39mpreprocess_train_val()\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got multiple values for argument 'tokenizer_class'"
     ]
    }
   ],
   "source": [
    "tokenizer=TokenizerClass()\n",
    "claim_extract=SentenceClassifier(df,tokenizer_class=tokenizer)\n",
    "claim_extract.preprocess_train_val()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claim_extract.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fang_text=fetch_from_fangcovid_local(5,10)\n",
    "#fang_pre=preprocess_text(fang_text)\n",
    "df_fang=pd.DataFrame()\n",
    "df_fang[\"text\"]=split_text(fang_text)\n",
    "df_fang[\"prediction\"]=claim_extract.predict_target(df_fang[\"text\"])\n",
    "\n",
    "# df_fang[\"tokenized\"]=tokenizer.texts_to_sequences(df_fang[\"sentence\"])\n",
    "# fang_padded = pad_sequences(df_fang[\"tokenized\"], maxlen=32, padding=\"post\", truncating=\"post\")\n",
    "# df_fang[\"predictions\"]=model.predict(fang_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_fang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "claim_extract.save_current_test_as_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles=['Wiesentalbrücke','Mammut (Gattung)','Riemannsche Vermutung','Reichstag zu Augsburg','Deutsch-Französischer Krieg','Kantonsspital Winterthur','Femizid','Nicht-zufällige Segregation von Chromosomen','Beryllium','Massenaussterben','Covid-19','Pandemie','Sex','Homosexualität','OG Keemo','Conchita Wurst','Hochschule für Medien, Kommunikation und Wirtschaft','Europäische Union','Biographie','Donald Trump','Angstzustände','Doktortitel','BAHN-BKK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles=[\"Maschinelles Lernen\", \"Medizin\",\"Wissenschaft\",\"Krankheit\",\"Prävention\",\"Diagnose\",\"Politik\",\"COVID-19\",\"COVID-19-Pandemie\",\"Epidemie\",\"Mykose\",\"Sexuell übertragbare Erkrankung\",\"Infektionskrankheit\",\"Bundestag\",\"Bundesrat\",\"Zeitung\",\"Rundfunk\",\"Verlag\",\"Politisches System der Bundesrepublik Deutschland\",\"Politisches System\",\"Massenmedien\",\"Medienwissenschaft\",\"Publikation\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wiesentalbrücke\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'fetch_rawtext_from_wiki' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m name \u001b[39min\u001b[39;00m articles:\n\u001b[1;32m      5\u001b[0m     \u001b[39mprint\u001b[39m(name)\n\u001b[0;32m----> 6\u001b[0m     raw\u001b[39m=\u001b[39mfetch_rawtext_from_wiki(name)\n\u001b[1;32m      7\u001b[0m     text\u001b[39m=\u001b[39mtext \u001b[39m+\u001b[39m raw\n\u001b[1;32m      8\u001b[0m df\u001b[39m=\u001b[39mpreprocess_classify_wiki_text(text)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fetch_rawtext_from_wiki' is not defined"
     ]
    }
   ],
   "source": [
    "# Loop over articles\n",
    "\n",
    "text=''\n",
    "for name in articles:\n",
    "    print(name)\n",
    "    raw=fetch_rawtext_from_wiki(name)\n",
    "    text=text + raw\n",
    "df=preprocess_classify_wiki_text(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline, if \"claims\" is free text of all labeled claims \n",
    "\n",
    "split_claims=split_text(claims)\n",
    "df_claim =pd.DataFrame()\n",
    "df_claim[\"text\"]= split_claims\n",
    "df_claim=df_claim.assign(target=True)\n",
    "df=df.loc[df[\"target\"]==False]\n",
    "df=pd.concat([df,df_claim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Re-Balanced Class Sets\n",
    "df_majority = df[df.target==0]\n",
    "df_minority = df[df.target==1]\n",
    " \n",
    "# Downsample majority class\n",
    "df_majority_downsampled = resample(df_majority, \n",
    "                                 replace=False,    # sample without replacement\n",
    "                                 n_samples=2451,     # to match minority class\n",
    "                                 random_state=123) # reproducible results\n",
    " \n",
    "# Combine minority class with downsampled majority class\n",
    "df = pd.concat([df_majority_downsampled, df_minority])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fang_text=fetch_from_fangcovid_local(5,10)\n",
    "#fang_pre=preprocess_text(fang_text)\n",
    "df_fang=pd.DataFrame()\n",
    "df_fang[\"sentence\"]=split_text(fang_text)\n",
    "df_fang[\"tokenized\"]=tokenizer.texts_to_sequences(df_fang[\"sentence\"])\n",
    "fang_padded = pad_sequences(df_fang[\"tokenized\"], maxlen=32, padding=\"post\", truncating=\"post\")\n",
    "df_fang[\"predictions\"]=model.predict(fang_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_fang[\"predictions\"]=model.predict(fang_padded)\n",
    "\n",
    "print(df_fang.sort_values(\"predictions\"))\n",
    "os.chdir(\"/Users/jannis/Desktop/fang-covid-main\")\n",
    "df_fang=df_fang[['predictions','sentence']]\n",
    "df_fang.to_csv(\"results7.csv\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_count={}\n",
    "claims=\" \"\n",
    "os.chdir(\"/Users/jannis/Desktop/x-fact-main/data/x-fact\")\n",
    "with open(\"train.all.tsv\", 'r') as fp:\n",
    "    for line in fp:\n",
    "        arr = line.strip().split('\\t')\n",
    "        lang = arr[1].lower()\n",
    "        site = arr[2].lower()\n",
    "        domain = (lang, site)\n",
    "        \n",
    "\n",
    "        if domain not in label_count:\n",
    "                label_count[domain] = {}\n",
    "        if arr[0] == 'de':\n",
    "            claims = claims + arr[3]\n",
    "            print(arr[3])\n",
    "            \n",
    "\n",
    "        label = arr[-1].lower()\n",
    "\n",
    "        if label not in label_count[domain]:\n",
    "            label_count[domain][label] = 0\n",
    "        label_count[domain][label] +=1\n",
    "\n",
    "\n",
    "\n",
    "# new_map = {}\n",
    "# for key in label_count.keys():\n",
    "#     new_map[key] = {}\n",
    "#     counts = label_count[key]\n",
    "\n",
    "#     total = 0\n",
    "#     for k, v in counts.items():\n",
    "#         total += v\n",
    "\n",
    "#     for k,v in counts.items():\n",
    "#         new_map[key][k] = float(v)/total\n",
    "\n",
    "\n",
    "# print(new_map)\n",
    "\n",
    "# total = 0\n",
    "# count = 0\n",
    "# for key in new_map.keys():\n",
    "#     counts = new_map[key]\n",
    "#     take = True\n",
    "#     for k, v in counts.items():\n",
    "#         if v > 0.7:\n",
    "#             take = False\n",
    "\n",
    "#     if take:\n",
    "#         print(key)\n",
    "#         count +=1\n",
    "#         for k, v in label_count[key].items():\n",
    "#             total += v\n",
    "\n",
    "# print(total)\n",
    "# print(count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"https://raw.githubusercontent.com/justusmattern/fang-covid/main/articles/20000.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"mlsum\", \"de\")\n",
    "df_json=pd.read_json(dataset[\"train\"])\n",
    "claim=\" \"\n",
    "i=0\n",
    "while i<10000:\n",
    "    claim=claim+dataset[\"train\"][\"summary\"][randint(0,220000)]\n",
    "    i=+1\n",
    "array=dataset[\"train\"][\"summary\"][:10000]\n",
    "s=\" \".join(array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=German()\n",
    "doc=nlp(text)\n",
    "nlp.add_pipe('sentencizer')\n",
    "sents=list(doc.sents)\n",
    "print(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    nlp = spacy.load(\"de_core_news_md\")\n",
    "    doc = nlp(sentences[0])\n",
    "    print(doc.text)\n",
    "    for token in doc:\n",
    "        print(token.text, token.pos_, token.dep_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sentence_class",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0 (default, Nov 15 2020, 06:25:35) \n[Clang 10.0.0 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ed4db69b2ad72da59ecb679942dc6ba3d277df9b74703a594d096c49c628851e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
