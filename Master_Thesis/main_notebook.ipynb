{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jannis/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import resample\n",
    "from tensorflow import keras\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "from random import sample\n",
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from spacy.lang.de import German\n",
    "from utils import *\n",
    "from sentence_classifier import SentenceClassifier\n",
    "from tokenizer_class import TokenizerClass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maschinelles Lernen\n",
      "Medizin\n",
      "Wissenschaft\n",
      "Krankheit\n",
      "Prävention\n",
      "Diagnose\n",
      "Politik\n",
      "COVID-19\n",
      "COVID-19-Pandemie\n",
      "Epidemie\n",
      "Mykose\n",
      "Sexuell übertragbare Erkrankung\n",
      "Infektionskrankheit\n",
      "Bundestag\n",
      "Bundesrat\n",
      "Zeitung\n",
      "Rundfunk\n",
      "Verlag\n",
      "Politisches System der Bundesrepublik Deutschland\n",
      "Politisches System\n",
      "Massenmedien\n",
      "Medienwissenschaft\n",
      "Publikation\n"
     ]
    }
   ],
   "source": [
    "articles=[\"Maschinelles Lernen\", \"Medizin\",\"Wissenschaft\",\"Krankheit\",\"Prävention\",\"Diagnose\",\"Politik\",\"COVID-19\",\"COVID-19-Pandemie\",\"Epidemie\",\"Mykose\",\"Sexuell übertragbare Erkrankung\",\"Infektionskrankheit\",\"Bundestag\",\"Bundesrat\",\"Zeitung\",\"Rundfunk\",\"Verlag\",\"Politisches System der Bundesrepublik Deutschland\",\"Politisches System\",\"Massenmedien\",\"Medienwissenschaft\",\"Publikation\"]\n",
    "# Loop over articles\n",
    "\n",
    "text=''\n",
    "for name in articles:\n",
    "    print(name)\n",
    "    raw=fetch_rawtext_from_wiki(name)\n",
    "    text=text + raw\n",
    "df=preprocess_classify_wiki_text(text)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected a 1D array, got an array with shape (3930, 526)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/envs/masterth-env/lib/python3.10/site-packages/pandas/core/indexes/base.py:3803\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3802\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3803\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3804\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/masterth-env/lib/python3.10/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/masterth-env/lib/python3.10/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'padded'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/envs/masterth-env/lib/python3.10/site-packages/pandas/core/frame.py:4138\u001b[0m, in \u001b[0;36mDataFrame._set_item_mgr\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4137\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 4138\u001b[0m     loc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_info_axis\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   4139\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[1;32m   4140\u001b[0m     \u001b[39m# This item wasn't present, just insert at end\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/masterth-env/lib/python3.10/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3806\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3807\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3808\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3809\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'padded'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m tokenizer\u001b[39m=\u001b[39mTokenizerClass()\n\u001b[1;32m      2\u001b[0m claim_extract\u001b[39m=\u001b[39mSentenceClassifier(df,tokenizer_class\u001b[39m=\u001b[39mtokenizer)\n\u001b[0;32m----> 3\u001b[0m claim_extract\u001b[39m.\u001b[39;49mpreprocess_train_val()\n",
      "File \u001b[0;32m~/Desktop/TryVSCode/Master_Thesis/sentence_classifier.py:41\u001b[0m, in \u001b[0;36mSentenceClassifier.preprocess_train_val\u001b[0;34m(self, text_column)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_df, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mval_df \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msplit_val_train(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwhole_df)\n\u001b[1;32m     40\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer_class\u001b[39m.\u001b[39mfit_tokenizer_on_train(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_df[text_column]\u001b[39m.\u001b[39mtolist())\n\u001b[0;32m---> 41\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_df[\u001b[39m\"\u001b[39m\u001b[39mpadded\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw_text_to_padded_sequences(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_df[\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m     42\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mval_df[\u001b[39m\"\u001b[39m\u001b[39mpadded\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw_text_to_padded_sequences(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mval_df[\u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m     43\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_df, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mval_df\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/masterth-env/lib/python3.10/site-packages/pandas/core/frame.py:3977\u001b[0m, in \u001b[0;36mDataFrame.__setitem__\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   3974\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_setitem_array([key], value)\n\u001b[1;32m   3975\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   3976\u001b[0m     \u001b[39m# set column\u001b[39;00m\n\u001b[0;32m-> 3977\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_set_item(key, value)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/masterth-env/lib/python3.10/site-packages/pandas/core/frame.py:4184\u001b[0m, in \u001b[0;36mDataFrame._set_item\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4181\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(existing_piece, DataFrame):\n\u001b[1;32m   4182\u001b[0m             value \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mtile(value, (\u001b[39mlen\u001b[39m(existing_piece\u001b[39m.\u001b[39mcolumns), \u001b[39m1\u001b[39m))\u001b[39m.\u001b[39mT\n\u001b[0;32m-> 4184\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_set_item_mgr(key, value)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/masterth-env/lib/python3.10/site-packages/pandas/core/frame.py:4141\u001b[0m, in \u001b[0;36mDataFrame._set_item_mgr\u001b[0;34m(self, key, value)\u001b[0m\n\u001b[1;32m   4138\u001b[0m     loc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_info_axis\u001b[39m.\u001b[39mget_loc(key)\n\u001b[1;32m   4139\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[1;32m   4140\u001b[0m     \u001b[39m# This item wasn't present, just insert at end\u001b[39;00m\n\u001b[0;32m-> 4141\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_mgr\u001b[39m.\u001b[39;49minsert(\u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_info_axis), key, value)\n\u001b[1;32m   4142\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   4143\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iset_item_mgr(loc, value)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/masterth-env/lib/python3.10/site-packages/pandas/core/internals/managers.py:1388\u001b[0m, in \u001b[0;36mBlockManager.insert\u001b[0;34m(self, loc, item, value)\u001b[0m\n\u001b[1;32m   1386\u001b[0m     value \u001b[39m=\u001b[39m value\u001b[39m.\u001b[39mT\n\u001b[1;32m   1387\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(value) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m-> 1388\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1389\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected a 1D array, got an array with shape \u001b[39m\u001b[39m{\u001b[39;00mvalue\u001b[39m.\u001b[39mT\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1390\u001b[0m         )\n\u001b[1;32m   1391\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1392\u001b[0m     value \u001b[39m=\u001b[39m ensure_block_shape(value, ndim\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mndim)\n",
      "\u001b[0;31mValueError\u001b[0m: Expected a 1D array, got an array with shape (3930, 526)"
     ]
    }
   ],
   "source": [
    "tokenizer=TokenizerClass()\n",
    "claim_extract=SentenceClassifier(df,tokenizer_class=tokenizer)\n",
    "claim_extract.preprocess_train_val()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ab=tokenizer.fit_tokenizer_on_train(claim_extract.whole_df[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles=['Wiesentalbrücke','Mammut (Gattung)','Riemannsche Vermutung','Reichstag zu Augsburg','Deutsch-Französischer Krieg','Kantonsspital Winterthur','Femizid','Nicht-zufällige Segregation von Chromosomen','Beryllium','Massenaussterben','Covid-19','Pandemie','Sex','Homosexualität','OG Keemo','Conchita Wurst','Hochschule für Medien, Kommunikation und Wirtschaft','Europäische Union','Biographie','Donald Trump','Angstzustände','Doktortitel','BAHN-BKK']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles=[\"Maschinelles Lernen\", \"Medizin\",\"Wissenschaft\",\"Krankheit\",\"Prävention\",\"Diagnose\",\"Politik\",\"COVID-19\",\"COVID-19-Pandemie\",\"Epidemie\",\"Mykose\",\"Sexuell übertragbare Erkrankung\",\"Infektionskrankheit\",\"Bundestag\",\"Bundesrat\",\"Zeitung\",\"Rundfunk\",\"Verlag\",\"Politisches System der Bundesrepublik Deutschland\",\"Politisches System\",\"Massenmedien\",\"Medienwissenschaft\",\"Publikation\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wiesentalbrücke\n",
      "Mammut (Gattung)\n",
      "Riemannsche Vermutung\n",
      "Reichstag zu Augsburg\n",
      "Deutsch-Französischer Krieg\n",
      "Kantonsspital Winterthur\n",
      "Femizid\n",
      "Nicht-zufällige Segregation von Chromosomen\n",
      "Beryllium\n",
      "Massenaussterben\n",
      "Covid-19\n",
      "Pandemie\n",
      "Sex\n",
      "Homosexualität\n",
      "OG Keemo\n",
      "Conchita Wurst\n",
      "Hochschule für Medien, Kommunikation und Wirtschaft\n",
      "Europäische Union\n",
      "Biographie\n",
      "Donald Trump\n",
      "Angstzustände\n",
      "Doktortitel\n",
      "BAHN-BKK\n"
     ]
    }
   ],
   "source": [
    "# Loop over articles\n",
    "\n",
    "text=''\n",
    "for name in articles:\n",
    "    print(name)\n",
    "    raw=fetch_rawtext_from_wiki(name)\n",
    "    text=text + raw\n",
    "df=preprocess_classify_wiki_text(text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline, if \"claims\" is free text of all labeled claims \n",
    "split_claims=split_text(claims)\n",
    "df_claim =pd.DataFrame()\n",
    "df_claim[\"text\"]= split_claims\n",
    "df_claim=df_claim.assign(target=True)\n",
    "df=df.loc[df[\"target\"]==False]\n",
    "df=pd.concat([df,df_claim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Re-Balanced Class Sets\n",
    "df_majority = df[df.target==0]\n",
    "df_minority = df[df.target==1]\n",
    " \n",
    "# Downsample majority class\n",
    "df_majority_downsampled = resample(df_majority, \n",
    "                                 replace=False,    # sample without replacement\n",
    "                                 n_samples=2451,     # to match minority class\n",
    "                                 random_state=123) # reproducible results\n",
    " \n",
    "# Combine minority class with downsampled majority class\n",
    "df = pd.concat([df_majority_downsampled, df_minority])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Further Pipeline to preprocess model\n",
    "df[\"text\"]=df.text.map(remove_stopwords)\n",
    "counter = counter_word(df.text)\n",
    "num_unique_words=len(counter)\n",
    "train_df,test_df = split_train_test(df)\n",
    "print(num_unique_words)\n",
    "tokenizer = Tokenizer(num_words=num_unique_words)\n",
    "tokenizer.fit_on_texts(train_df[\"text\"]) \n",
    "train_sequences = tokenizer.texts_to_sequences(train_df['text'])\n",
    "val_sequences = tokenizer.texts_to_sequences(test_df['text'])\n",
    "train_padded = pad_sequences(train_sequences, maxlen=32, padding=\"post\", truncating=\"post\")\n",
    "val_padded = pad_sequences(val_sequences, maxlen=32, padding=\"post\", truncating=\"post\")\n",
    "word_index = tokenizer.word_index\n",
    "reverse_word_index = dict([(idx, word) for (word, idx) in word_index.items()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LSTM model\n",
    "from tensorflow.keras import layers, callbacks\n",
    "from keras.callbacks import EarlyStopping\n",
    "# Embedding: https://www.tensorflow.org/tutorials/text/word_embeddings\n",
    "# Turns positive integers (indexes) into dense vectors of fixed size. (other approach could be one-hot-encoding)\n",
    "\n",
    "# Word embeddings give us a way to use an efficient, dense representation in which similar words have \n",
    "# a similar encoding. Importantly, you do not have to specify this encoding by hand. An embedding is a \n",
    "# dense vector of floating point values (the length of the vector is a parameter you specify).\n",
    "\n",
    "es = EarlyStopping('val_loss',mode='min',verbose=1,patience =2)\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(layers.Embedding(num_unique_words, 32, input_length=32))\n",
    "\n",
    "# The layer will take as input an integer matrix of size (batch, input_length),\n",
    "# and the largest integer (i.e. word index) in the input should be no larger than num_words (vocabulary size).\n",
    "# Now model.output_shape is (None, input_length, 32), where `None` is the batch dimension.\n",
    "\n",
    "\n",
    "model.add(layers.LSTM(64, dropout=0.1))\n",
    "\n",
    "model.add(layers.Dense(1, activation=\"sigmoid\"))\n",
    "#TODO try out others than sigmoid\n",
    "#min 2nd layer or more\n",
    "\n",
    "model.summary()\n",
    "loss = keras.losses.BinaryCrossentropy(from_logits=False)\n",
    "optim = keras.optimizers.Adam(learning_rate=0.001)\n",
    "metrics = [\"accuracy\"]\n",
    "\n",
    "model.compile(loss=loss, optimizer=optim, metrics=metrics)\n",
    "model.fit(train_padded, train_df[\"target\"], epochs=20, validation_data=(val_padded, test_df[\"target\"]), verbose=2, callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fang_text=fetch_from_fangcovid_local(5,10)\n",
    "#fang_pre=preprocess_text(fang_text)\n",
    "df_fang=pd.DataFrame()\n",
    "df_fang[\"sentence\"]=split_text(fang_text)\n",
    "df_fang[\"tokenized\"]=tokenizer.texts_to_sequences(df_fang[\"sentence\"])\n",
    "fang_padded = pad_sequences(df_fang[\"tokenized\"], maxlen=32, padding=\"post\", truncating=\"post\")\n",
    "df_fang[\"predictions\"]=model.predict(fang_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_fang[\"predictions\"]=model.predict(fang_padded)\n",
    "\n",
    "print(df_fang.sort_values(\"predictions\"))\n",
    "os.chdir(\"/Users/jannis/Desktop/fang-covid-main\")\n",
    "df_fang=df_fang[['predictions','sentence']]\n",
    "df_fang.to_csv(\"results7.csv\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_count={}\n",
    "claims=\" \"\n",
    "os.chdir(\"/Users/jannis/Desktop/x-fact-main/data/x-fact\")\n",
    "with open(\"train.all.tsv\", 'r') as fp:\n",
    "    for line in fp:\n",
    "        arr = line.strip().split('\\t')\n",
    "        lang = arr[1].lower()\n",
    "        site = arr[2].lower()\n",
    "        domain = (lang, site)\n",
    "        \n",
    "\n",
    "        if domain not in label_count:\n",
    "                label_count[domain] = {}\n",
    "        if arr[0] == 'de':\n",
    "            claims = claims + arr[3]\n",
    "            print(arr[3])\n",
    "            \n",
    "\n",
    "        label = arr[-1].lower()\n",
    "\n",
    "        if label not in label_count[domain]:\n",
    "            label_count[domain][label] = 0\n",
    "        label_count[domain][label] +=1\n",
    "\n",
    "\n",
    "\n",
    "# new_map = {}\n",
    "# for key in label_count.keys():\n",
    "#     new_map[key] = {}\n",
    "#     counts = label_count[key]\n",
    "\n",
    "#     total = 0\n",
    "#     for k, v in counts.items():\n",
    "#         total += v\n",
    "\n",
    "#     for k,v in counts.items():\n",
    "#         new_map[key][k] = float(v)/total\n",
    "\n",
    "\n",
    "# print(new_map)\n",
    "\n",
    "# total = 0\n",
    "# count = 0\n",
    "# for key in new_map.keys():\n",
    "#     counts = new_map[key]\n",
    "#     take = True\n",
    "#     for k, v in counts.items():\n",
    "#         if v > 0.7:\n",
    "#             take = False\n",
    "\n",
    "#     if take:\n",
    "#         print(key)\n",
    "#         count +=1\n",
    "#         for k, v in label_count[key].items():\n",
    "#             total += v\n",
    "\n",
    "# print(total)\n",
    "# print(count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(claims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text=fetch_from_fangcovid(5)\n",
    "print(len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"https://raw.githubusercontent.com/justusmattern/fang-covid/main/articles/20000.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"mlsum\", \"de\")\n",
    "df_json=pd.read_json(dataset[\"train\"])\n",
    "claim=\" \"\n",
    "i=0\n",
    "while i<10000:\n",
    "    claim=claim+dataset[\"train\"][\"summary\"][randint(0,220000)]\n",
    "    i=+1\n",
    "array=dataset[\"train\"][\"summary\"][:10000]\n",
    "s=\" \".join(array)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=German()\n",
    "doc=nlp(text)\n",
    "nlp.add_pipe('sentencizer')\n",
    "sents=list(doc.sents)\n",
    "print(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    nlp = spacy.load(\"de_core_news_md\")\n",
    "    doc = nlp(sentences[0])\n",
    "    print(doc.text)\n",
    "    for token in doc:\n",
    "        print(token.text, token.pos_, token.dep_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('masterth-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aeea6e7f45dc14f171f4c3978cea6ad3ef16857108b0c24174dd8089e5da7144"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
