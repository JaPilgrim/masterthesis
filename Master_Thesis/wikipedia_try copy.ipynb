{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import torch\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dbmdz/bert-base-german-cased\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"dbmdz/bert-base-german-cased\", num_labels=2)\n",
    "#model = AutoModel.from_pretrained(\"dbmdz/bert-base-german-cased\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-21 16:28:13.233600: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Some weights of the model checkpoint at dbmdz/bert-base-german-cased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type distilbert to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n",
      "Some weights of the model checkpoint at distilbert-base-german-cased were not used when initializing BertForSequenceClassification: ['vocab_layer_norm.weight', 'distilbert.transformer.layer.3.attention.k_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.bias', 'distilbert.transformer.layer.0.attention.v_lin.bias', 'distilbert.transformer.layer.2.ffn.lin1.weight', 'distilbert.transformer.layer.2.attention.q_lin.weight', 'distilbert.transformer.layer.3.attention.v_lin.bias', 'distilbert.transformer.layer.2.attention.out_lin.bias', 'distilbert.transformer.layer.1.attention.v_lin.weight', 'distilbert.transformer.layer.5.ffn.lin1.weight', 'distilbert.transformer.layer.5.attention.out_lin.bias', 'distilbert.transformer.layer.2.attention.k_lin.bias', 'distilbert.transformer.layer.0.attention.k_lin.bias', 'distilbert.transformer.layer.3.ffn.lin1.bias', 'distilbert.transformer.layer.4.ffn.lin1.bias', 'distilbert.transformer.layer.4.sa_layer_norm.weight', 'distilbert.transformer.layer.2.output_layer_norm.bias', 'distilbert.transformer.layer.0.attention.v_lin.weight', 'distilbert.transformer.layer.4.attention.v_lin.weight', 'distilbert.transformer.layer.4.ffn.lin2.bias', 'distilbert.transformer.layer.4.attention.q_lin.bias', 'distilbert.transformer.layer.5.attention.k_lin.weight', 'distilbert.transformer.layer.3.ffn.lin1.weight', 'distilbert.embeddings.position_embeddings.weight', 'distilbert.transformer.layer.2.attention.k_lin.weight', 'distilbert.transformer.layer.1.sa_layer_norm.bias', 'distilbert.transformer.layer.0.attention.out_lin.weight', 'distilbert.transformer.layer.3.attention.k_lin.bias', 'distilbert.transformer.layer.4.attention.out_lin.weight', 'distilbert.transformer.layer.3.attention.q_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.bias', 'distilbert.transformer.layer.2.attention.q_lin.bias', 'distilbert.transformer.layer.2.attention.v_lin.weight', 'distilbert.transformer.layer.0.output_layer_norm.bias', 'distilbert.transformer.layer.0.attention.q_lin.bias', 'distilbert.transformer.layer.1.attention.k_lin.bias', 'distilbert.transformer.layer.2.ffn.lin2.weight', 'distilbert.transformer.layer.2.attention.v_lin.bias', 'distilbert.transformer.layer.1.output_layer_norm.weight', 'distilbert.transformer.layer.2.sa_layer_norm.weight', 'distilbert.transformer.layer.4.attention.v_lin.bias', 'distilbert.embeddings.LayerNorm.weight', 'distilbert.transformer.layer.1.ffn.lin1.bias', 'vocab_transform.bias', 'vocab_projector.bias', 'distilbert.transformer.layer.0.output_layer_norm.weight', 'distilbert.transformer.layer.4.attention.out_lin.bias', 'vocab_layer_norm.bias', 'distilbert.transformer.layer.0.sa_layer_norm.bias', 'distilbert.transformer.layer.0.ffn.lin2.weight', 'distilbert.transformer.layer.1.ffn.lin1.weight', 'distilbert.transformer.layer.2.ffn.lin1.bias', 'distilbert.transformer.layer.5.ffn.lin2.bias', 'distilbert.transformer.layer.1.sa_layer_norm.weight', 'distilbert.transformer.layer.1.ffn.lin2.bias', 'distilbert.transformer.layer.5.attention.k_lin.bias', 'distilbert.transformer.layer.1.ffn.lin2.weight', 'distilbert.transformer.layer.5.attention.out_lin.weight', 'distilbert.transformer.layer.5.sa_layer_norm.bias', 'vocab_projector.weight', 'distilbert.transformer.layer.1.attention.v_lin.bias', 'distilbert.transformer.layer.5.attention.v_lin.weight', 'distilbert.transformer.layer.3.sa_layer_norm.bias', 'distilbert.transformer.layer.5.sa_layer_norm.weight', 'distilbert.transformer.layer.2.sa_layer_norm.bias', 'distilbert.transformer.layer.5.ffn.lin2.weight', 'distilbert.transformer.layer.1.output_layer_norm.bias', 'distilbert.transformer.layer.0.attention.q_lin.weight', 'distilbert.transformer.layer.3.ffn.lin2.bias', 'distilbert.transformer.layer.4.sa_layer_norm.bias', 'distilbert.transformer.layer.1.attention.k_lin.weight', 'distilbert.transformer.layer.3.attention.out_lin.weight', 'distilbert.transformer.layer.2.attention.out_lin.weight', 'distilbert.transformer.layer.0.attention.k_lin.weight', 'distilbert.transformer.layer.1.attention.out_lin.weight', 'distilbert.transformer.layer.5.output_layer_norm.weight', 'distilbert.transformer.layer.2.output_layer_norm.weight', 'distilbert.transformer.layer.4.ffn.lin2.weight', 'distilbert.transformer.layer.0.ffn.lin2.bias', 'distilbert.transformer.layer.5.attention.q_lin.weight', 'distilbert.transformer.layer.3.attention.out_lin.bias', 'vocab_transform.weight', 'distilbert.transformer.layer.3.ffn.lin2.weight', 'distilbert.transformer.layer.1.attention.q_lin.bias', 'distilbert.transformer.layer.3.attention.v_lin.weight', 'distilbert.transformer.layer.4.output_layer_norm.weight', 'distilbert.transformer.layer.4.output_layer_norm.bias', 'distilbert.transformer.layer.4.attention.q_lin.weight', 'distilbert.transformer.layer.3.output_layer_norm.bias', 'distilbert.transformer.layer.4.ffn.lin1.weight', 'distilbert.transformer.layer.5.attention.v_lin.bias', 'distilbert.transformer.layer.0.ffn.lin1.bias', 'distilbert.embeddings.word_embeddings.weight', 'distilbert.transformer.layer.0.sa_layer_norm.weight', 'distilbert.transformer.layer.5.ffn.lin1.bias', 'distilbert.transformer.layer.0.attention.out_lin.bias', 'distilbert.transformer.layer.2.ffn.lin2.bias', 'distilbert.transformer.layer.4.attention.k_lin.bias', 'distilbert.transformer.layer.5.attention.q_lin.bias', 'distilbert.transformer.layer.5.output_layer_norm.bias', 'distilbert.transformer.layer.4.attention.k_lin.weight', 'distilbert.transformer.layer.3.output_layer_norm.weight', 'distilbert.transformer.layer.0.ffn.lin1.weight', 'distilbert.transformer.layer.1.attention.q_lin.weight', 'distilbert.transformer.layer.3.sa_layer_norm.weight', 'distilbert.embeddings.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-german-cased and are newly initialized: ['encoder.layer.10.intermediate.dense.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.6.attention.self.value.weight', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.9.output.dense.weight', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.11.attention.self.value.bias', 'pooler.dense.weight', 'encoder.layer.7.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.self.key.bias', 'encoder.layer.5.attention.self.query.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.11.output.LayerNorm.bias', 'pooler.dense.bias', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.6.output.dense.bias', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.4.attention.self.key.bias', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.9.attention.self.key.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.2.attention.self.value.bias', 'embeddings.word_embeddings.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.10.output.dense.weight', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.3.output.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.1.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.self.query.weight', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.attention.self.query.bias', 'encoder.layer.4.attention.self.value.weight', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.0.output.LayerNorm.weight', 'embeddings.LayerNorm.bias', 'encoder.layer.6.attention.self.query.weight', 'classifier.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.8.intermediate.dense.bias', 'embeddings.token_type_embeddings.weight', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.10.attention.output.dense.weight', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.5.output.dense.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.6.attention.self.query.bias', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.11.attention.self.key.weight', 'embeddings.LayerNorm.weight', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'classifier.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.8.attention.output.LayerNorm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'DistilBertTokenizer'. \n",
      "The class this function is called from is 'BertTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def fetch_rawtext_from_wiki(subject='Maschinelles Lernen') :\n",
    "     \n",
    "    url = 'https://de.wikipedia.org/w/api.php'\n",
    "    params = {\n",
    "                'action': 'parse',\n",
    "                'page': subject,\n",
    "                'format': 'json',\n",
    "                'prop':'text',\n",
    "                'redirects':''\n",
    "            }\n",
    "    \n",
    "    response = requests.get(url, params=params)\n",
    "    data = response.json()\n",
    "    \n",
    "    raw_html = data['parse']['text']['*']\n",
    "    soup = BeautifulSoup(raw_html,'html.parser')\n",
    "    soup.find_all('p')\n",
    "    text = ''\n",
    "    \n",
    "    for p in soup.find_all('p'):\n",
    "        text += p.text\n",
    "    return text\n",
    "\n",
    "def preprocess_text(text):\n",
    "    newtext=re.sub(\"\\[.*?\\]\",\"_\",text)\n",
    "    i=0\n",
    "    newtext=newtext.replace(\"._\",\"_.\")\n",
    "    while i<1:\n",
    "        newtext=newtext.replace(\"._\",\".\")\n",
    "        i+=1\n",
    "    return newtext\n",
    "\n",
    "def split_classify_text(text):\n",
    "    sen_text=text.split('.')\n",
    "    is_claim=[]\n",
    "    for i in sen_text:\n",
    "        if \"_\" in i:\n",
    "            is_claim.append(True)\n",
    "        else:\n",
    "            is_claim.append(False)\n",
    "    df=pd.DataFrame()\n",
    "    df[\"sentence_raw\"]=sen_text\n",
    "    df[\"is_claim\"]=is_claim\n",
    "\n",
    "\n",
    "    return df\n",
    "\n",
    "def split_train_test(df):\n",
    "    train_df, test_df = train_test_split(df,test_size=.10)\n",
    "\n",
    "    return train_df,test_df\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run Classes\n",
    "text=fetch_rawtext_from_wiki()\n",
    "text_pre=preprocess_text(text)\n",
    "df=split_classify_text(text_pre)\n",
    "train_df,test_df = split_train_test(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jannis/opt/anaconda3/envs/masterth-env/lib/python3.9/site-packages/simpletransformers/classification/classification_model.py:612: UserWarning: Dataframe headers not specified. Falling back to using column 0 as text and column 1 as labels.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99e1b862243641719a69c2d297561994",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/44 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-21 16:02:27.705222: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9650aaeed868475cae9224ac9e3e28a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c64cb1516ccb43008d5d46068ede5a7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running Epoch 0 of 4:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model.train_model(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7bb4de70a4d403e8a656e769d269e94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb8dc110c0434a518a6892e9f9aa1d2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/433 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42fdaf07cf0b4888b0f93f04fabe91cc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/255k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25657ed176e24c66a6748eec2f11099d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/485k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ImportError",
     "evalue": "\nAutoModelForMaskedLM requires the PyTorch library but it was not found in your environment.\nHowever, we were able to find a TensorFlow installation. TensorFlow classes begin\nwith \"TF\", but are otherwise identically named to our PyTorch classes. This\nmeans that the TF equivalent of the class you tried to import would be \"TFAutoModelForMaskedLM\".\nIf you want to use TensorFlow, please use TF classes instead!\n\nIf you really do want to use PyTorch please go to\nhttps://pytorch.org/get-started/locally/ and follow the instructions that\nmatch your environment.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/xr/3j03brc91h94wkpmlmrnhjlc0000gn/T/ipykernel_35018/709113828.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bert-base-german-cased\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModelForMaskedLM\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bert-base-german-cased\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/envs/masterth-env/lib/python3.9/site-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(cls, key)\u001b[0m\n\u001b[1;32m    990\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"_\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 992\u001b[0;31m         \u001b[0mrequires_backends\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backends\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    993\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    994\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/masterth-env/lib/python3.9/site-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36mrequires_backends\u001b[0;34m(obj, backends)\u001b[0m\n\u001b[1;32m    969\u001b[0m     \u001b[0;31m# Raise an error for users who might not realize that classes without \"TF\" are torch-only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    970\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m\"torch\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbackends\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"tf\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbackends\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_torch_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_tf_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 971\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPYTORCH_IMPORT_ERROR_WITH_TF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    972\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    973\u001b[0m     \u001b[0;31m# Raise the inverse error for PyTorch users trying to load TF classes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: \nAutoModelForMaskedLM requires the PyTorch library but it was not found in your environment.\nHowever, we were able to find a TensorFlow installation. TensorFlow classes begin\nwith \"TF\", but are otherwise identically named to our PyTorch classes. This\nmeans that the TF equivalent of the class you tried to import would be \"TFAutoModelForMaskedLM\".\nIf you want to use TensorFlow, please use TF classes instead!\n\nIf you really do want to use PyTorch please go to\nhttps://pytorch.org/get-started/locally/ and follow the instructions that\nmatch your environment.\n"
     ]
    }
   ],
   "source": [
    "# from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"bert-base-german-cased\")\n",
    "\n",
    "# model = AutoModelForMaskedLM.from_pretrained(\"bert-base-german-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(text):\n",
    "    nlp = spacy.load(\"de_core_news_md\")\n",
    "    doc = nlp(sentences[0])\n",
    "    print(doc.text)\n",
    "    for token in doc:\n",
    "        print(token.text, token.pos_, token.dep_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('masterth-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aeea6e7f45dc14f171f4c3978cea6ad3ef16857108b0c24174dd8089e5da7144"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
