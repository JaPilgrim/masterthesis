{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jannis/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from utils import *\n",
    "\n",
    "import string\n",
    "from collections import Counter\n",
    "import spacy\n",
    "\n",
    "\n",
    "\n",
    "# Loop over articles\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'4.64.1'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib.metadata as importlib_metadata\n",
    "importlib_metadata.version('tqdm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/sentence10/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-02-06 13:00:55.585637: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nlp = spacy.load(\"de_core_news_sm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/jannis/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from utils import *\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maschinelles Lernen\n",
      "Medizin\n",
      "Wissenschaft\n",
      "Krankheit\n",
      "Pr채vention\n",
      "Diagnose\n",
      "Politik\n",
      "COVID-19\n",
      "COVID-19-Pandemie\n",
      "Epidemie\n",
      "Mykose\n",
      "Sexuell 체bertragbare Erkrankung\n",
      "Infektionskrankheit\n",
      "Bundestag\n",
      "Bundesrat\n",
      "Zeitung\n",
      "Rundfunk\n",
      "Verlag\n",
      "Politisches System der Bundesrepublik Deutschland\n",
      "Politisches System\n",
      "Massenmedien\n",
      "Medienwissenschaft\n",
      "Publikation\n"
     ]
    }
   ],
   "source": [
    "articles=[\"Maschinelles Lernen\", \"Medizin\",\"Wissenschaft\",\"Krankheit\",\"Pr채vention\",\"Diagnose\",\"Politik\",\"COVID-19\",\"COVID-19-Pandemie\",\"Epidemie\",\"Mykose\",\"Sexuell 체bertragbare Erkrankung\",\"Infektionskrankheit\",\"Bundestag\",\"Bundesrat\",\"Zeitung\",\"Rundfunk\",\"Verlag\",\"Politisches System der Bundesrepublik Deutschland\",\"Politisches System\",\"Massenmedien\",\"Medienwissenschaft\",\"Publikation\"]\n",
    "text=''\n",
    "for name in articles:\n",
    "    print(name)\n",
    "    raw=fetch_rawtext_from_wiki(name)\n",
    "    text=text + raw\n",
    "df=preprocess_classify_wiki_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_rem = df.text.map(remove_stopwords)\n",
    "punct_rem = words_rem.map(lambda x:x.translate(str.maketrans('','',string.punctuation)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_list=[]\n",
    "for sentence in punct_rem:\n",
    "    doc = nlp(sentence)\n",
    "    sentence_pos =\"\"\n",
    "    for token in doc:\n",
    "        sentence_pos=sentence_pos+\" \"+token.pos_\n",
    "    pos_list.append(sentence_pos)\n",
    "df[\"text\"] = pos_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentence_pos(sentence)->list[str]:\n",
    "    doc = nlp(sentence)\n",
    "    sentence_pos = []\n",
    "    for token in doc:\n",
    "        sentence_pos.append(str(token.pos_))\n",
    "    return sentence_pos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_list=[]\n",
    "for sentence in punct_rem:\n",
    "    \n",
    "    a = get_sentence_pos(sentence)\n",
    "    pos_list.append(a)\n",
    "df[\"pos\"] = pos_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"pos\"] = pos_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to add remove Numbers & Remove Punctuation here!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct_rem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iterates over each sentence, applies nlp individually and then adds up string of POS tags of a sentence, than appends it to list. TAKE ABOUT 9 Minutes!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(punct_rem[24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"pos\"] = pos_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "claim_pos_list=list(df[df[\"target\"]==True][\"pos\"])\n",
    "noclaim_pos_list=list(df[df[\"target\"]==False][\"pos\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_claim=(df[df[\"target\"]==True])\n",
    "dataframe_noclaim=(df[df[\"target\"]==False])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gives most frequent pos combis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_noclaim[\"pos\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " X                                                                      120\n",
      " NUM X                                                                   42\n",
      " NOUN                                                                    27\n",
      " NOUN NOUN NOUN NOUN VERB                                                12\n",
      " NOUN NOUN VERB                                                          10\n",
      "                                                                       ... \n",
      " NOUN NUM AUX PROPN NOUN PROPN X                                          1\n",
      " PROPN VERB ADJ NOUN X                                                    1\n",
      " NUM ADV NUM PROPN PROPN PROPN X                                          1\n",
      " NUM PROPN ADV ADJ X                                                      1\n",
      " SPACE NOUN NOUN PROPN PUNCT ADJ NOUN NOUN ADV ADV ADJ NOUN DET NOUN      1\n",
      "Name: pos, Length: 2877, dtype: int64\n",
      " NUM NOUN VERB ADV NUM ADJ NOUN                                                            3\n",
      " NOUN VERB                                                                                 3\n",
      " NOUN ADV ADJ NOUN NOUN                                                                    2\n",
      " NOUN NOUN NOUN ADV                                                                        2\n",
      " NUM NOUN NUM VERB X NUM ADJ NUM NOUN                                                      2\n",
      "                                                                                          ..\n",
      " NOUN NOUN NOUN NOUN VERB NOUN ADV AUX NOUN ADJ NOUN ADJ PROPN NOUN VERB NOUN NOUN VERB    1\n",
      " ADV NOUN NOUN CCONJ ADJ NOUN NOUN NOUN VERB                                               1\n",
      " ADJ NOUN ADJ NOUN VERB NOUN NOUN NOUN ADJ NOUN NOUN ADJ NOUN                              1\n",
      " ADP ADJ NOUN PROPN NOUN ADV NOUN NOUN ADP ADJ ADJ NOUN ADJ NOUN ADJ NOUN VERB             1\n",
      " NOUN ADP ADV X VERB CCONJ NOUN NOUN ADV NOUN ADJ NOUN                                     1\n",
      "Name: pos, Length: 1061, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(dataframe_noclaim[\"pos\"].value_counts())\n",
    "print(dataframe_claim[\"pos\"].value_counts())\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "iterrates throguh noclaim list, and prints out every sentence, whiches pos is matched with any pos of claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=len(noclaim_pos_list)\n",
    "j=0\n",
    "while(j<i):\n",
    "    if(noclaim_pos_list[j] in claim_pos_list):\n",
    "        print(dataframe_noclaim[\"text\"].iloc[j])\n",
    "    j=j+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[\"text\"].iloc[1:5])\n",
    "sentences = list(df[\"text\"].iloc[1:5])\n",
    "paraphrases = util.paraphrase_mining(model,sentences)\n",
    "for paraphrase in paraphrases:\n",
    "    score, i, j = paraphrase\n",
    "    print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences[i], sentences[j], score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = list(df[\"text\"].iloc[1:5])\n",
    "embeddings = model.encode(sentences)\n",
    "for sentence, embedding in zip(sentences, embeddings):\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Embedding:\", embedding)\n",
    "    print(\"\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
