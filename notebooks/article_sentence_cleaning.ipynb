{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import statistics\n",
    "import requests\n",
    "import concurrent.futures\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import ast\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_dict = {\n",
    "    'protected': {\n",
    "        'df':\n",
    "        pd.read_csv(\n",
    "            f'data/data_files/pipeline_steps/protected_articles/1_all_articles_fetched.csv'),\n",
    "        'titles':\n",
    "        pd.read_csv(f'data/data_files/pipeline_steps/protected_articles/0_protected_titles.csv')\n",
    "    },\n",
    "    'excellent': {\n",
    "        'df':\n",
    "        pd.read_csv(\n",
    "            f'data/data_files/pipeline_steps/excellent_articles/1_all_articles_fetched.csv'),\n",
    "        'titles':\n",
    "        pd.read_csv(f'data/data_files/pipeline_steps/excellent_articles/0_excellent_titles.csv')\n",
    "    },\n",
    "    'readworthy': {\n",
    "        'df':\n",
    "        pd.read_csv(\n",
    "            f'data/data_files/pipeline_steps/readworthy_articles/1_all_articles_fetched.csv'),\n",
    "        'titles':\n",
    "        pd.read_csv(f'data/data_files/pipeline_steps/readworthy_articles/0_readworthy_titles.csv')\n",
    "    },\n",
    "    'random': {\n",
    "        'df':\n",
    "        pd.read_csv(f'data/data_files/pipeline_steps/random_articles/1_all_articles_fetched.csv'),\n",
    "        'titles': pd.read_csv(f'data/data_files/pipeline_steps/random_articles/0_random_titles.csv')\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_list =['protected','excellent','readworthy','random']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_dict ={}\n",
    "for origin in article_list:\n",
    "    whole_dict[origin] ={}\n",
    "    whole_dict[origin]['df_fetched'] = pd.read_csv(\n",
    "        f'data/data_files/pipeline_steps/{origin}_articles/1_all_articles_fetched.csv')\n",
    "    whole_dict[origin]['df_split'] = pd.read_csv(\n",
    "        f'data/data_files/pipeline_steps/{origin}_articles/2_articles_labeled_cleaned.csv')\n",
    "    whole_dict[origin]['df_split']['sentence_list'] = whole_dict[origin]['df_split']['sentence_list'].apply(ast.literal_eval)\n",
    "    whole_dict[origin]['titles'] = pd.read_csv(\n",
    "        f'data/data_files/pipeline_steps/{origin}_articles/0_{origin}_titles.csv')\n",
    "    whole_dict[origin]['df_final'] = pd.read_csv(\n",
    "        f'data/data_files/pipeline_steps/{origin}_articles/7.1_maskadded.csv')\n",
    "    whole_dict[origin]['df_resolved'] = pd.read_csv(\n",
    "        f'data/data_files/pipeline_steps/excellent_articles/4_pos_resolved_truth_cleaned.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_lists(list1, list2):\n",
    "    # Convert the lists to sets\n",
    "    set1 = set(list1)\n",
    "    set2 = set(list2)\n",
    "\n",
    "    # Find the intersection and difference\n",
    "    intersection = set1 & set2\n",
    "    difference = (set1 | set2) - intersection\n",
    "\n",
    "    # Print the counts\n",
    "    # print(f'Number of matches: {len(intersection)}')\n",
    "    # print(f'Number of non-matches: {len(difference)}')\n",
    "    return len(difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in whole_dict.items():\n",
    "    df = whole_dict[key]['df_resolved']\n",
    "    for i,row in df.iterrows():\n",
    "        res_list = row['sentence_list']\n",
    "        nores_list = row['resolved_sentence_list']\n",
    "        b =compare_lists(res_list,nores_list)\n",
    "        if b!=0:\n",
    "            print(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of matches: 138206\n",
      "Number of non-matches: 191495\n"
     ]
    }
   ],
   "source": [
    "compare_lists(res_list,nores_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = whole_dict['protected']['df_resolved']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['title', 'bytes', 'resolved_text_list', 'cleaned_article_text',\n",
       "       'sub_texts', 'resolved_sentence_list', 'sentence_list',\n",
       "       'quot_truth_list', 'link_truth_list', 'linkname_truth_list',\n",
       "       'pos_sentence_list', 'pos_resolved_sentence_list'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_dict['protected']['df_resolved'].columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Origin: protected\n",
      " Before processing had:\n",
      " 2845 articles\n",
      " 335584 sentences\n",
      " After processing it had: \n",
      " 2531 articles\n",
      " 289843 sentences\n",
      " \n",
      " \n",
      " Origin: excellent\n",
      " Before processing had:\n",
      " 2778 articles\n",
      " 675838 sentences\n",
      " After processing it had: \n",
      " 2693 articles\n",
      " 590756 sentences\n",
      " \n",
      " \n",
      " Origin: readworthy\n",
      " Before processing had:\n",
      " 4324 articles\n",
      " 748814 sentences\n",
      " After processing it had: \n",
      " 4226 articles\n",
      " 651668 sentences\n",
      " \n",
      " \n",
      " Origin: random\n",
      " Before processing had:\n",
      " 50000 articles\n",
      " 870312 sentences\n",
      " After processing it had: \n",
      " 39704 articles\n",
      " 707478 sentences\n",
      " \n",
      " \n"
     ]
    }
   ],
   "source": [
    "for key,value in whole_dict.items():\n",
    "    print(F\" Origin: {key}\")\n",
    "    print(F\" Before processing had:\")\n",
    "    print(F\" {len(whole_dict[key]['df_fetched'])} articles\")\n",
    "    total_sentences_pre = whole_dict[key]['df_split']['sentence_list'].apply(len).sum()\n",
    "    print(F\" {total_sentences_pre} sentences\")\n",
    "    print(F\" After processing it had: \")\n",
    "    total_articles_post = whole_dict[key]['df_final']['article_index'].nunique()\n",
    "    print(F\" {total_articles_post} articles\")\n",
    "    print(F\" {len(whole_dict[key]['df_final'])} sentences\")\n",
    "\n",
    "    print(\" \")\n",
    "    print(\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jannis-env9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
